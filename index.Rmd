---
title: "R for Data Science Exercise Solutions"
author: "Ben Herbertson"
date: "Updated 25 Apr 2018"
output:
  rmdformats::html_clean
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(tidyverse)
```

---

# 3. Data visualisation

## 3.2.4 Exercises

1.  Run `ggplot(data = mpg)`. What do you see?

```{r}
ggplot(data = mpg)
```

Nothing. The plot is empty because no layers have been added to `ggplot()`.

2.  How many rows are in `mpg`? How many columns?

```{r}
mpg
```

234 rows and 11 columns in total.

3.  What does the `drv` variable describe?  Read the help for `?mpg` to find
    out.

`drv` describes whether the car is front-wheel drive, rear wheel drive, or 4wd.

4.  Make a scatterplot of `hwy` vs `cyl`.

```{r}
ggplot(mpg, aes(x = hwy, y = cyl)) +
  geom_point()
```

5.  What happens if you make a scatterplot of `class` vs `drv`? Why is
    the plot not useful?

```{r}
ggplot(mpg, aes(x = class, y = drv)) +
  geom_point()
```

Scatterplots are useful for displaying continuous variables (e.g., `cty` and `hwy`). `class` and `drv` are categorical variables.

## 3.3.1 Exercises

1.  What's gone wrong with this code? Why are the points not blue?

    ```{r}
    ggplot(data = mpg) + 
      geom_point(mapping = aes(x = displ, y = hwy, color = "blue"))
    ```
    
To make the points blue, colour must be set manually (i.e., it must be located *outside* `aes()`):

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), color = "blue")
```

2.  Which variables in `mpg` are categorical? Which variables are continuous? 
    (Hint: type `?mpg` to read the documentation for the dataset). How
    can you see this information when you run `mpg`?
    
```{r}
mpg
```

Categorical: `manufacturer`, `model`, `trans`, `drv`, `fl`, `class`  
Continuous: `displ`, `year`, `cty`, `hwy`

This information is located below the variable name in the output (e.g., `<chr>` indicates a character string which is categorical).

3.  Map a continuous variable to `color`, `size`, and `shape`. How do
    these aesthetics behave differently for categorical vs. continuous
    variables?
    
4.  What happens if you map the same variable to multiple aesthetics?

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, colour = displ))
```

It creates a gradient along whichever axis the variable is assigned to.

5.  What does the `stroke` aesthetic do? What shapes does it work with?
    (Hint: use `?geom_point`)
    
`stroke` increases the border width of shapes. However, not all shapes have a border.

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), stroke = 2, shape = 23)
```
    
6.  What happens if you map an aesthetic to something other than a variable 
    name, like `aes(colour = displ < 5)`?
    
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, colour = displ < 5))
```
  
In this example, each observation with `displ < 5` is grouped together (`TRUE`). All remaining observations form a second group (`FALSE`).

# 4. Workflow: basics

## 4.4 Practice

1.  Why does this code not work?

    ```{r, error = TRUE}
    my_variable <- 10
    my_varıable
    ```
    
    Look carefully! (This may seem like an exercise in pointlessness, but
    training your brain to notice even the tiniest difference will pay off
    when programming.)
    
There is a typo on the second line. It should be `my_variable`, not `my_varıable`.
    
2.  Tweak each of the following R commands so that they run correctly:

    ```{r, eval = FALSE}
    library(tidyverse)

    ggplot(dota = mpg) + 
      geom_point(mapping = aes(x = displ, y = hwy))
    
    fliter(mpg, cyl = 8)
    filter(diamond, carat > 3)
    ```

```{r}
library(tidyverse)

ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy))

filter(mpg, cyl == 8)
filter(diamonds, carat > 3)
```

3.  Press Alt + Shift + K. What happens? How can you get to the same place
    using the menus?
    
*The Keyboard Shortcut Quick Reference* overlay appears.

# 5. Data transformation

```{r, include = FALSE}
library(nycflights13)
```

## 5.2.4 Exercises

1.  Find all flights that

    1. Had an arrival delay of two or more hours
    2. Flew to Houston (`IAH` or `HOU`)
    3. Were operated by United, American, or Delta
    4. Departed in summer (July, August, and September)
    5. Arrived more than two hours late, but didn't leave late
    6. Were delayed by at least an hour, but made up over 30 minutes in flight
    7. Departed between midnight and 6am (inclusive)
    
Had an arrival delay of two or more hours:

```{r}
filter(flights, arr_delay >= 120)
```

Flew to Houston (`IAH` or `HOU`):

```{r}
filter(flights, dest %in% c("IAH", "HOU"))
```

Were operated by United, American, or Delta:

```{r}
filter(flights, carrier %in% c("UA", "AA", "DL"))
```
(To find the two letter carrier abbreviations, see `airlines`.)

Departed in summer (July, August, and September):

```{r}
filter(flights, month %in% c(7, 8, 9))
```

Arrived more than two hours late, but didn't leave late:

```{r}
filter(flights, arr_delay >= 120 & dep_delay <= 0)
```

Were delayed by at least an hour, but made up over 30 minutes in flight:

```{r}
filter(flights, dep_delay >= 60  & dep_delay - arr_delay > 30)
```

Departed between midnight and 6am (inclusive):

```{r}
filter(flights, dep_time <= 600 | dep_time == 2400)
```

2.  Another useful dplyr filtering helper is `between()`. What does it do?
    Can you use it to simplify the code needed to answer the previous 
    challenges?
    
It is a shortcut for finding values that fall within a specified range:

```{r}
filter(flights, between(month, 7, 9))
```

3.  How many flights have a missing `dep_time`? What other variables are 
    missing? What might these rows represent?
    
```{r}
filter(flights, is.na(dep_time))
```

8,255 flights.

To see what other variables are missing, you can use `View()`:

```{r, eval=FALSE}
View(filter(flights, is.na(dep_time)))
```

`dep_delay`, `arr_time`, `arr_delay`, and `air_time` are also missing. These flights were most likely cancelled.

4.  Why is `NA ^ 0` not missing? Why is `NA | TRUE` not missing?
    Why is `FALSE & NA` not missing? Can you figure out the general
    rule?  (`NA * 0` is a tricky counterexample!)
    
See Jeffrey Arnold's [explanation](https://jrnold.github.io/r4ds-exercise-solutions/data-transformation.html#filter).

## 5.3.1 Exercises

1.  How could you use `arrange()` to sort all missing values to the start?
    (Hint: use `is.na()`).
    
```{r}
arrange(flights, desc(is.na(dep_time)))
```
    
2.  Sort `flights` to find the most delayed flights. Find the flights that
    left earliest.
    
```{r}
arrange(flights, desc(dep_delay))
```

```{r}
arrange(flights, dep_delay)
```

3.  Sort `flights` to find the fastest flights.

This depends how you define "fastest flights". It could refer to the least amount of `air_time`:

```{r}
arrange(flights, air_time)
```

4.  Which flights travelled the longest? Which travelled the shortest?

If we assume this refers to distance travelled:

```{r}
arrange(flights, desc(distance))
```

The longest flight is from JFK to HNL (Honolulu) at 4983 miles.

```{r}
arrange(flights, distance)
```

The shortest flight is from EWR (Newark) to LGA (La Guardia) at 17 miles, however this appears to be a cancelled flight (a number of variables such as `dep_time` are missing). The next shortest flight is from EWR to PHL (Philadelphia) at 80 miles.

(See `airports` to find the names that correspond to the FAA airport code.)

## 5.4.1 Exercises

1.  Brainstorm as many ways as possible to select `dep_time`, `dep_delay`,
    `arr_time`, and `arr_delay` from `flights`.
    
```{r}
select(flights, dep_time, dep_delay, arr_time, arr_delay)

select(flights, starts_with("dep"), starts_with("arr"))
```
    
2.  What happens if you include the name of a variable multiple times in
    a `select()` call?
    
It will only print once.

```{r}
select(flights, dep_time, dep_time)
```
  
3.  What does the `one_of()` function do? Why might it be helpful in conjunction
    with this vector?
    
    ```{r}
    vars <- c("year", "month", "day", "dep_delay", "arr_delay")
    ```

The `one_of()` function allows you to use variables stored in a character vector:

```{r}
select(flights, one_of(vars))
```

It improves the readability of code.
    
4.  Does the result of running the following code surprise you?  How do the
    select helpers deal with case by default? How can you change that default?

    ```{r, eval = FALSE}
    select(flights, contains("TIME"))
    ```

Select helpers are not case sensitive by default. To change the default, use `ignore.case = FALSE`.

## 5.5.2 Exercises

```{r, eval = FALSE, echo = FALSE}
flights <- flights %>% mutate(
  dep_time = hour * 60 + minute,
  arr_time = (arr_time %/% 100) * 60 + (arr_time %% 100),
  airtime2 = arr_time - dep_time,
  dep_sched = dep_time + dep_delay
)

ggplot(flights, aes(dep_sched)) + geom_histogram(binwidth = 60)
ggplot(flights, aes(dep_sched %% 60)) + geom_histogram(binwidth = 1)
ggplot(flights, aes(air_time - airtime2)) + geom_histogram()
```

1.  Currently `dep_time` and `sched_dep_time` are convenient to look at, but
    hard to compute with because they're not really continuous numbers. 
    Convert them to a more convenient representation of number of minutes
    since midnight.
    
```{r}
transmute(flights, 
  dep_time,
  hour = dep_time %/% 100,
  minute = dep_time %% 100,
  dep_time_mins = (hour * 60) + minute
)

transmute(flights, 
  sched_dep_time,
  hour = sched_dep_time %/% 100,
  minute = sched_dep_time %% 100,
  sched_dep_mins = (hour * 60) + minute
)
```   
    
2.  Compare `air_time` with `arr_time - dep_time`. What do you expect to see?
    What do you see? What do you need to do to fix it?
    
```{r}
transmute(flights, air_time, arr_time - dep_time)
```

You would expect `arr_time - dep_time == air_time`. This discrepancy is explained by timezone differences. `arr_time` and `dep_time` are in local timezones that may differ.
    
3.  Compare `dep_time`, `sched_dep_time`, and `dep_delay`. How would you
    expect those three numbers to be related?
    
```{r}
select(flights, dep_time, sched_dep_time, dep_delay)
```

`dep_time - sched_dep_time == dep_delay`.

4.  Find the 10 most delayed flights using a ranking function. How do you want 
    to handle ties? Carefully read the documentation for `min_rank()`.
    
```{r}
flights_ranked <- mutate(flights, rank = min_rank(-dep_delay))
arrange(flights_ranked, desc(dep_delay))
```

Ties should be assigned the lowest rank. `ties.method = "min"` is the default for `min_rank()`.

5.  What does `1:3 + 1:10` return? Why?

```{r, eval=TRUE}
1:3 + 1:10
```

1 + 1 = 2; 2 + 2 = 4; 3 + 3 = 6; 4 + 1 = 5; 5 + 2 = 7, and so on.

6.  What trigonometric functions does R provide?

Cosine, sine, tangent, arc-cosine, and more. See `?Trig`.

## 5.6.7 Exercises

1.  Brainstorm at least 5 different ways to assess the typical delay 
    characteristics of a group of flights. Consider the following scenarios:
    
    * A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of 
      the time.
      
    * A flight is always 10 minutes late.

    * A flight is 30 minutes early 50% of the time, and 30 minutes late 50% of 
      the time.
      
    * 99% of the time a flight is on time. 1% of the time it's 2 hours late.
    
    Which is more important: arrival delay or departure delay?
    
```{r}
flights %>% 
  group_by(flight, origin, dest) %>% 
  filter(arr_delay == 10) %>% 
  summarise()
```

```{r}
flights %>% 
  group_by(flight, origin, dest) %>%
  summarise(avg_arr_delay = mean(arr_delay, na.rm = TRUE), count = n()) %>% 
  filter(avg_arr_delay == 10)
```

Departure delay is probably more important for travellers.

2.  Come up with another approach that will give you the same output as 
    `not_cancelled %>% count(dest)` and 
    `not_cancelled %>% count(tailnum, wt = distance)` (without using 
    `count()`).
    
```{r, include=FALSE}
not_cancelled <- flights %>% 
  filter(!is.na(dep_delay), !is.na(arr_delay))
```

```{r}
not_cancelled %>% 
  group_by(dest) %>% 
  summarise(count = n())

not_cancelled %>% 
  group_by(tailnum) %>% 
  summarise(distance_total = sum(distance))
```

3.  Our definition of cancelled flights (`is.na(dep_delay) | is.na(arr_delay)`
    ) is slightly suboptimal. Why? Which is the most important column?
    
Flights that do no depart cannot arrive at any location. However, flights that depart but are `NA` for `arr_delay` may have redirected or crashed.

The importance of the column depends on the question being answered. `dep_delay` is probably more representative of cancelled flights for the above reason.

4.  Look at the number of cancelled flights per day. Is there a pattern?
    Is the proportion of cancelled flights related to the average delay?
    
```{r}
flights %>% 
  group_by(month, day) %>% 
  summarise(cancelled = sum(is.na(dep_delay)))
```

It is difficult to tell just from this output. Let's visualise this instead:

```{r, include=TRUE}
flights %>% 
  mutate(cancelled = is.na(dep_delay)) %>% 
  group_by(month, day) %>% 
  summarise(
    prop_cancelled = mean(cancelled),
    avg_dep_delay = mean(dep_delay, na.rm = TRUE)
  ) %>% 
  ggplot(aes(x = prop_cancelled, y = avg_dep_delay)) +
    geom_point() +
    geom_smooth(se = TRUE)
```

There is a positive relationship between the proportion of cancelled flights and average delay. However, there are a few exceptional days with a high number of cancelled flights and relatively low average delay. If I had to guess, this might be related to bad weather (i.e., all flights were cancelled for a specific time period, so `avg_dep_delay` was close to zero).

5.  Which carrier has the worst delays? Challenge: can you disentangle the
    effects of bad airports vs. bad carriers? Why/why not? (Hint: think about
    `flights %>% group_by(carrier, dest) %>% summarise(n())`)
    
Frontier Airlines (F9) has the worst delays:

```{r}
not_cancelled %>% 
  group_by(carrier) %>% 
  summarise(avg_delay = mean(dep_delay, na.rm = TRUE)) %>% 
  arrange(desc(avg_delay))
```

You cannot disentangle the effects of bad airports vs. bad carriers because there is an uneven distribution in destinations by carrier. Some carriers might operate more frequently from bad airports and vice versa.

6.  What does the `sort` argument to `count()` do. When might you use it?

`sort = TRUE` will sort output in descending order of `n`. You can use it for ranking instead of using `min_rank()` and/or `arrange()`.

```{r}
not_cancelled %>% count(dest, sort = TRUE)
```

## 5.7.1 Exercises

1.  Refer back to the lists of useful mutate and filtering functions. 
    Describe how each operation changes when you combine it with grouping.
    
When combined with grouping, functions operate within groups only, not the entire dataset. For example:

```{r}
# What is the mean air time for each carrier?
flights %>%
  group_by(carrier) %>% 
  summarise(mean_air_time = mean(air_time, na.rm = TRUE))
```

```{r}
# What is the mean arrival delay for each month excluding all flights that were not delayed?
flights %>% 
  filter(arr_delay > 0) %>% 
  group_by(month) %>%
  summarise(avg_arr_delay = mean(arr_delay))
```

2.  Which plane (`tailnum`) has the worst on-time record?

N844MH.

```{r}
flights %>% 
  group_by(tailnum) %>%
  summarise(avg_delay = mean(arr_delay, na.rm = TRUE)) %>% 
  arrange(desc(avg_delay))
```

3.  What time of day should you fly if you want to avoid delays as much
    as possible?
    
Morning, preferably before 9 am.

```{r}
flights %>%
  group_by(hour) %>%
  summarise(avg_delay = mean(arr_delay, na.rm = TRUE)) %>% 
  arrange(avg_delay)
```
    
4.  For each destination, compute the total minutes of delay. For each, 
    flight, compute the proportion of the total delay for its destination.
    
```{r}
flights %>% 
  group_by(dest) %>% 
  summarise(total_delay = sum(arr_delay, na.rm = TRUE))
```
    
5.  Delays are typically temporally correlated: even once the problem that
    caused the initial delay has been resolved, later flights are delayed 
    to allow earlier flights to leave. Using `lag()` explore how the delay
    of a flight is related to the delay of the immediately preceding flight.
    
```{r}
flights %>% 
  group_by(year, month, day) %>% 
  mutate(lag_delay = lag(dep_delay)) %>% 
  ggplot(aes(x = dep_delay, y = lag_delay)) +
    geom_point() +
    geom_smooth()
```
    
6.  Look at each destination. Can you find flights that are suspiciously
    fast? (i.e. flights that represent a potential data entry error). Compute
    the air time a flight relative to the shortest flight to that destination.
    Which flights were most delayed in the air?
    
7.  Find all destinations that are flown by at least two carriers. Use that
    information to rank the carriers.
    
```{r}
flights %>% 
  group_by(dest, carrier) %>% 
  count(carrier) %>% 
  filter(carrier >= 2) %>% 
  group_by(carrier) %>% 
  count(sort = TRUE)
```

8.  For each plane, count the number of flights before the first delay 
    of greater than 1 hour.
    
```{r}
flights %>% 
  group_by(flight) %>% 
  filter(dep_delay <= 60) %>% 
  count()
```

# 6. Workflow: scripts

## 6.3 Practice

1.  Go to the RStudio Tips twitter account, <https://twitter.com/rstudiotips>
    and find one tip that looks interesting. Practice using it!
    
Ctrl + 1 (focus editor) and Ctrl + 2 (focus console) are my personal favourites.

2.  What other common mistakes will RStudio diagnostics report?  Read
    <https://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics> to 
    find out.
    
Common mistakes such as:

* Warn if variable used has no definition in scope  
* Warn if variable is defined but not used  
* Style

Also, diagnostics for other languages such as C++, JavaScript, and Python.

# 7. Exploratory Data Analysis

```{r, include = FALSE}
library(tidyverse)
library(ggstance)
library(lvplot)
library(ggbeeswarm)
library(nycflights13)
```

## 7.3.4 Exercises

1.  Explore the distribution of each of the `x`, `y`, and `z` variables 
    in `diamonds`. What do you learn? Think about a diamond and how you
    might decide which dimension is the length, width, and depth.

```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = x), binwidth = 0.5)

ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = x), binwidth = 0.1) +
  coord_cartesian(ylim = c(0, 500))

diamonds %>% 
  count(cut_width(x, 0.5))

ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = y), binwidth = 0.5)

ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = y), binwidth = 0.5) +
  coord_cartesian(ylim = c(0, 50))

diamonds %>% 
  count(cut_width(y, 0.5))

ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = z), binwidth = 0.5)

ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = z), binwidth = 0.1) +
  coord_cartesian(ylim = c(0, 50))

diamonds %>% 
  count(cut_width(z, 0.5))
```

There are a number of outliers in each distribution that require further investigation.

As seen when you compute the bin widths and corresponding counts by hand, the most common value for `x` and `y` is between 4.25 and 4.75. For `z`, the most common value is between 3.75 and 4.25. The bin width from 2.75 to 3.25 is also very common. Based on these values, `x` and `y` appear to be the length and width of the diamond, while `z` is most likely the depth. This is confirmed in the documentation (`?diamonds`).

2.  Explore the distribution of `price`. Do you discover anything unusual
    or surprising? (Hint: Carefully think about the `binwidth` and make sure
    you try a wide range of values.)
    
```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = price))

ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = price), binwidth = 10)
```

There appears to be no diamonds with a price around $1300, as indicated by the missing gap in the histogram above. This is not consistent with the overall pattern.

3.  How many diamonds are 0.99 carat? How many are 1 carat? What
    do you think is the cause of the difference?
    
```{r}
diamonds %>% 
  count(carat) %>% 
  filter(carat %in% c(0.99, 1))
```

23 diamonds are 0.99 carat. In comparison, there are over 1,558 diamonds that are 1 carat.

Maybe 0.99 carat diamonds are more likely to be rounded up.

```{r}
diamonds %>% 
  filter(carat == 0.99) %>% 
  select(carat, cut, price, x, y, z)

diamonds %>% 
  filter(carat == 1) %>% 
  select(carat, cut, price, x, y, z) %>% 
  ggplot(mapping = aes(x = cut)) +
  geom_bar()

diamonds %>% 
  filter(carat == 1) %>% 
  select(price) %>% 
  summary(price)

diamonds %>% 
  filter(carat == 1) %>% 
  select(carat, cut, price, x, y, z) %>% 
  ggplot(mapping = aes(x = price)) +
  geom_histogram(binwidth = 250)
```
    
4.  Compare and contrast `coord_cartesian()` vs `xlim()` or `ylim()` when
    zooming in on a histogram. What happens if you leave `binwidth` unset?
    What happens if you try and zoom so only half a bar shows?
    
```{r}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5) +
  coord_cartesian(ylim = c(0, 50))

ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5) +
  ylim(0, 50)
```

`xlim()` and `ylim()` throw away data outside the limits.

```{r}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y)) +
  coord_cartesian(ylim = c(0, 50))

ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y)) +
  ylim(0, 50)
```

If `binwidth` is unset, it is easier to discern the bars when zooming in on a plot.

If you try to zoom with `xlim()` or `ylim()` so only half a bar shows, the data in that binwidth will be thrown away. So, the bar will disappear from the plot.

## 7.4.1 Exercises

1.  What happens to missing values in a histogram?  What happens to missing
    values in a bar chart? Why is there a difference?
    
In a histogram and bar chart, by default, missing values are removed with a warning. So, there is no actual difference as the question indicates.

2.  What does `na.rm = TRUE` do in `mean()` and `sum()`?

It removes missing values before the computation proceeds.

## 7.5.1.1 Exercises

1.  Use what you've learned to improve the visualisation of the departure times
    of cancelled vs. non-cancelled flights.
    
```{r}
flights %>%
  mutate(
    cancelled = is.na(dep_time),
    sched_hour = sched_dep_time %/% 100,
    sched_min = sched_dep_time %% 100,
    sched_dep_time = sched_hour + sched_min / 60
  ) %>% 
  ggplot(mapping = aes(sched_dep_time)) + 
    geom_boxplot(mapping = aes(x = cancelled, y = sched_dep_time))
```

2.  What variable in the diamonds dataset is most important for predicting
    the price of a diamond? How is that variable correlated with cut?
    Why does the combination of those two relationships lead to lower quality
    diamonds being more expensive?
    
Return to this question after completing the section on modelling. If I had to guess, I think `carat` might be the most important for predicting the price of a diamond. There is an exponential relationship between `carat` and `price`:

```{r}
ggplot(diamonds, mapping = aes(x = carat, y = price)) +
  geom_point(alpha = 1/100)
```

3.  Install the ggstance package, and create a horizontal boxplot.
    How does this compare to using `coord_flip()`?
    
`ggstance` provides flipped versions of geoms, stats, and positions. This means you can provide aesthetics in their natural order, making the code more readable:

```{r}
ggplot(data = mpg) +
  geom_boxploth(mapping = aes(x = hwy, y = reorder(class, hwy, FUN = median)))
```

In comparsion, `coord_flip()` can only flip the plot as a whole.

4.  One problem with boxplots is that they were developed in an era of 
    much smaller datasets and tend to display a prohibitively large
    number of "outlying values". One approach to remedy this problem is
    the letter value plot. Install the lvplot package, and try using
    `geom_lv()` to display the distribution of price vs cut. What
    do you learn? How do you interpret the plots?
    
```{r}
ggplot(diamonds) +
  geom_boxplot(mapping = aes(x = cut, y = price))
```

The boxplots show many outliers, which is typical for large sample sizes (*n > 1000*).

```{r}
ggplot(data = diamonds) +
  geom_lv(mapping = aes(x = cut, y = price))
```

Using the more precise (for *n > 1000*) letter-value box plots, there are less outliers. With letter-value box plots, the fattest box indicates the IQR.

5.  Compare and contrast `geom_violin()` with a facetted `geom_histogram()`,
    or a coloured `geom_freqpoly()`. What are the pros and cons of each 
    method?
    
```{r}
ggplot(diamonds) +
  geom_violin(mapping = aes(x = cut, y = price))

ggplot(diamonds) +
  geom_histogram(mapping = aes(x = price), binwidth = 1000) +
  facet_wrap(~ cut)
```

`geom_violin()` displays everything in one plot. However, it only shows the variability within the `cut` variable. In comparison, the facetted `geom_histogram()` shows the overall count for each binwidth. So, from the facetted histogram, it can be seen that not many diamonds have `cut == Fair` compared to, say, `cut == Ideal`.

6.  If you have a small dataset, it's sometimes useful to use `geom_jitter()`
    to see the relationship between a continuous and categorical variable.
    The ggbeeswarm package provides a number of methods similar to 
    `geom_jitter()`. List them and briefly describe what each one does.
    
ggbeeswarm provides two methods to reduce overplotting: `geom_quasirandom` and `geom_beeswarm`. Both methods show the density of the data at each point, while still showing each individual point.

```{r}
ggplot(iris,aes(Species, Sepal.Length)) +
  geom_point()

ggplot(iris,aes(Species, Sepal.Length)) +
  geom_jitter()

ggplot(iris, mapping = aes(Species, Sepal.Length)) +
  geom_quasirandom()

ggplot(iris, mapping = aes(Species, Sepal.Length)) +
  geom_beeswarm()
```

`geom_quasirandom` spaces dots using a [van der Corput sequence](https://en.wikipedia.org/wiki/Van_der_Corput_sequence) or Tukey texturing. In comparison, `geom_beeswarm` uses the beeswarm library.

## 7.5.2.1 Exercises

1.  How could you rescale the count dataset above to more clearly show
    the distribution of cut within colour, or colour within cut?
    
```{r}
diamonds %>% 
  count(cut, color)

diamonds %>% 
  count(color, cut) %>%  
  ggplot(mapping = aes(x = color, y = cut)) +
    geom_tile(mapping = aes(fill = n))

diamonds %>% 
  count(cut, color) %>%  
  ggplot(mapping = aes(x = color, y = cut)) +
    geom_tile(mapping = aes(fill = n))
```

2.  Use `geom_tile()` together with dplyr to explore how average flight
    delays vary by destination and month of year.  What makes the 
    plot difficult to read? How could you improve it?

```{r}
flights %>%
  group_by(month, dest) %>%
  summarise(avg_delay = mean(dep_delay, na.rm = TRUE)) %>%
  ggplot(mapping = aes(x = factor(month), y = dest)) +
    geom_tile(mapping = aes(fill = avg_delay))
```

The plot is difficult to read because there are many destinations on the y-axis. There are also missing values that could be removed to improve readability.

3.  Why is it slightly better to use `aes(x = color, y = cut)` rather
    than `aes(x = cut, y = color)` in the example above?
    
The larger counts are located in the top left which makes it more readable.

# 8. Workflow: projects

No exercises.

# 10. Tibbles

```{r, include = FALSE}
library(nycflights13)
```

## 10.5 Exercises

1.  How can you tell if an object is a tibble? (Hint: try printing `mtcars`,
    which is a regular data frame).

`# A tibble:` is displayed in the top left of the output. Also, additional information is printed, such as column types and total number of rows and columns.

```{r}
as_tibble(mtcars)
```

2.  Compare and contrast the following operations on a `data.frame` and 
    equivalent tibble. What is different? Why might the default data frame
    behaviours cause you frustration?
    
    ```{r, eval = FALSE}
    df <- data.frame(abc = 1, xyz = "a")
    df$x
    df[, "xyz"]
    df[, c("abc", "xyz")]
    ```

```{r}
df <- tibble(abc = 1, xyz = "a")
df$x
df[, "xyz"]
df[, c("abc", "xyz")]
```

The default data frame uses partial matching. In comparison, tibbles do not. Data frames also store strings as factors by default.

```{r}
df <- data.frame(abc = 1, xyz = "a")
glimpse(df)
```

3.  If you have the name of a variable stored in an object, e.g. `var <- "mpg"`,
    how can you extract the reference variable from a tibble?

By using the double bracket, `[[`. For example, `df[[var]]`.

4.  Practice referring to non-syntactic names in the following data frame by:

    1.  Extracting the variable called `1`.

    2.  Plotting a scatterplot of `1` vs `2`.

    3.  Creating a new column called `3` which is `2` divided by `1`.
        
    4.  Renaming the columns to `one`, `two` and `three`. 
    
    ```{r}
    annoying <- tibble(
      `1` = 1:10,
      `2` = `1` * 2 + rnorm(length(`1`))
    )
    ```

Extracting the variable called `1`:

```{r}
annoying$`1`
```

Plotting a scatterplot of `1` vs `2`:

```{r}
ggplot(annoying, aes(`1`, `2`)) +
  geom_point()
```

Creating a new column called `3` which is `2` divided by `1`:

```{r}
annoying <- annoying %>%
  mutate(`3` = `2` / `1`)
```

Renaming the columns to `one`, `two` and `three`:

```{r}
annoying <- annoying %>%
  rename(`one` = `1`,
         `two` = `2`,
         `three` = `3`)
```

5.  What does `tibble::enframe()` do? When might you use it?

It converts atomic vectors or lists to two-column data frames.

6.  What option controls how many additional column names are printed
    at the footer of a tibble?

`n_extra` from `print.tbl()`. For example:

```{r}
print(flights, n_extra = 1)
```

# 11. Data import

## 11.2.2 Exercises

1.  What function would you use to read a file where fields were separated with  
    "|"?

`read_delim()`. The `delim` argument can be used to separate fields with "|".

2.  Apart from `file`, `skip`, and `comment`, what other arguments do
    `read_csv()` and `read_tsv()` have in common?

Some other arguments they have in common: `col_names`, `col_types`, `locale`, and `na`. In fact, it appears they share the exact same arguments.

3.  What are the most important arguments to `read_fwf()`?

`widths`, `start`, and `end`.

4.  Sometimes strings in a CSV file contain commas. To prevent them from
    causing problems they need to be surrounded by a quoting character, like
    `"` or `'`. By convention, `read_csv()` assumes that the quoting
    character will be `"`, and if you want to change it you'll need to
    use `read_delim()` instead. What arguments do you need to specify
    to read the following text into a data frame?
    
    ```{r, eval = FALSE}
    "x,y\n1,'a,b'"
    ```

```{r}
x <- "x,y\n1,'a,b'"
df <- read_delim(x, ",", quote = "'")
```

5.  Identify what is wrong with each of the following inline CSV files. 
    What happens when you run the code?
    
    ```{r, eval = FALSE}
    read_csv("a,b\n1,2,3\n4,5,6")
    read_csv("a,b,c\n1,2\n1,2,3,4")
    read_csv("a,b\n\"1")
    read_csv("a,b\n1,2\na,b")
    read_csv("a;b\n1;3")
    ```

Let's fix these:

```{r}
read_csv("a,b,c\n1,2,3\n4,5,6")
```

```{r}
read_csv("a,b,c\n1,2,1\n2,3,4")
```

```{r, warning=FALSE}
read_csv("a,b\n1")
```

It's unclear what the intent was here:

```{r}
read_csv("a,b\n1,2\na,b")
```

```{r}
read_csv("a,b\n1,3")
```

## 11.3.5 Exercises

1.  What are the most important arguments to `locale()`?

`date_names` and maybe `decimal_mark`.

2.  What happens if you try and set `decimal_mark` and `grouping_mark` 
    to the same character? What happens to the default value of 
    `grouping_mark` when you set `decimal_mark` to ","? What happens
    to the default value of `decimal_mark` when you set the `grouping_mark`
    to "."?

```{r, error=TRUE}
locale(decimal_mark = ",", grouping_mark = ",")
```

It produces an error.

```{r}
locale(decimal_mark = ",")
```

The default value for `grouping_mark` is changed to ".".

```{r}
locale(grouping_mark = ".")
```

The default value for `decimal_mark` is changed to ",".

3.  I didn't discuss the `date_format` and `time_format` options to
    `locale()`. What do they do? Construct an example that shows when 
    they might be useful.

`date_format` and `time_format` allows you to specify the default date and time formats.

```{r}
str(parse_guess("01/13/2018", locale = locale(date_format = "%m/%d/%Y")))
```

According to `vignette("readr")`, `time_format` isn't currently used for anything.

4.  If you live outside the US, create a new locale object that encapsulates 
    the settings for the types of file you read most commonly.

```{r}
locale(date_names = "en", date_format = "%d/%m/%Y", decimal_mark = ".",
       grouping_mark = ",", tz = "Australia/Perth")
```

5.  What's the difference between `read_csv()` and `read_csv2()`?

`read_csv2` uses ";" (i.e., a semi-colon) for separators instead of "," (a comma).

6.  What are the most common encodings used in Europe? What are the
    most common encodings used in Asia? Do some googling to find out.

UTF-8 is the standard worldwide. In Europe, ISO 8859-1 is a common encoding. Shift JIS is a common encoding in Asia.

7.  Generate the correct format string to parse each of the following 
    dates and times:

```{r}
d1 <- "January 1, 2010"
d2 <- "2015-Mar-07"
d3 <- "06-Jun-2017"
d4 <- c("August 19 (2015)", "July 1 (2015)")
d5 <- "12/30/14" # Dec 30, 2014
t1 <- "1705"
t2 <- "11:15:10.12 PM"
```

```{r}
str(parse_date(d1, format = "%B %d, %Y"))
str(parse_date(d2, format = "%Y-%b-%d"))
str(parse_date(d3, format = "%d-%b-%Y"))
str(parse_date(d4, format = "%B %d (%Y)"))
str(parse_date(d5, format = "%m/%d/%y"))
str(parse_time(t1, format = "%H%M"))
str(parse_time(t2, format = "%H:%M:%OS %p"))
```

# 12. Tidy data

## 12.2.1 Exercises

1.  Using prose, describe how the variables and observations are organised in
    each of the sample tables.

In `table1`, each variable has its own column, each observation its own row, and each value its own cell. Therefore, `table1` is a tidy dataset.

In `table2` the `cases` and `population` do not have their own columns, but rather, are treated as values. `table3` does not have the `cases` and `population` variables stored separately. Instead, both are stored as part of a calculation under `rate`. `table4a` stores the values of the `year` variable as separate variables (with their own columns), with the values being the number of cases (population is missing). `table 4b` is similar, but uses `population` for the values rather than `cases`.

2.  Compute the `rate` for `table2`, and `table4a` + `table4b`. 
    You will need to perform four operations:

    1.  Extract the number of TB cases per country per year.
    2.  Extract the matching population per country per year.
    3.  Divide cases by population, and multiply by 10000.
    4.  Store back in the appropriate place.
    
    Which representation is easiest to work with? Which is hardest? Why?
    
Extract the number of TB cases per country per year:

```{r}
table2_cases <- filter(table2, type == "cases")[["count"]]
```

Extract the matching population per country per year:

```{r}
table2_population <- filter(table2, type == "population")[["count"]]
```

Divide cases by population, and multiply by 10000:

```{r}
table2_rate <- table2_cases / table2_population * 10000
```

Store back in the appropriate place:

```{r}
table2_country <- filter(table2, type == "cases")[["country"]]
table2_year <- filter(table2, type == "cases")[["year"]]

table2_new <- tibble(
  country = table2_country,
  year = table2_year,
  cases = table2_cases,
  population = table2_population,
  rate = table2_rate
  )
table2_new
```

For `table4a` and `table4b`:

```{r}
table4_new <- tibble(
  country = table4a[["country"]],
  `1999` = table4a[["1999"]] / table4b[["1999"]] * 10000,
  `2000` = table4a[["2000"]] / table4b[["2000"]] * 10000
  )
table4_new
```

3.  Recreate the plot showing change in cases over time using `table2`
    instead of `table1`. What do you need to do first?

Filter `cases`:

```{r}
table2 %>%
  filter(type == "cases") %>% 
  ggplot(aes(year, count, group = country, colour = country)) +
    geom_line() +
    geom_point()
```

## 12.3.3 Exercises

1.  Why are `gather()` and `spread()` not perfectly symmetrical?  
    Carefully consider the following example:
    
    ```{r, results = 'hide'}
    stocks <- tibble(
      year   = c(2015, 2015, 2016, 2016),
      half  = c(   1,    2,     1,    2),
      return = c(1.88, 0.59, 0.92, 0.17)
    )
    stocks %>% 
      spread(year, return) %>% 
      gather("year", "return", `2015`:`2016`)
    ```
    
    (Hint: look at the variable types and think about column _names_.)
    
    Both `spread()` and `gather()` have a `convert` argument. What does it 
    do?

`gather()` and `spread()` are not perfectly symmetrical because variable type is not retained when using both functions, as seen in the example above. In `stocks`, `year` is numeric:

```{r}
stocks %>% select(year)
```

However, after using the `spread()` and `gather()` functions it becomes `character`:

```{r}
stocks %>% 
  spread(year, return) %>% 
  gather("year", "return", `2015`:`2016`) %>% 
  select(year)
```

The `convert` argument, when set to `TRUE`, will retain numeric, integer, or logical column types when using `spread()` or `gather()`.

2.  Why does this code fail?

    ```{r, error = TRUE}
    table4a %>% 
      gather(1999, 2000, key = "year", value = "cases")
    ```

"1999" and "2000" are non-syntactic names so they have to be surrounded by backticks.

```{r}
table4a %>% 
  gather(`1999`, `2000`, key = "year", value = "cases")
```

3.  Why does spreading this tibble fail? How could you add a new column to fix
    the problem?

    ```{r}
    people <- tribble(
      ~name,             ~key,    ~value,
      #-----------------|--------|------
      "Phillip Woods",   "age",       45,
      "Phillip Woods",   "height",   186,
      "Phillip Woods",   "age",       50,
      "Jessica Cordero", "age",       37,
      "Jessica Cordero", "height",   156
    )
    ```

```{r, error = TRUE}
people %>% spread(key, value)
```

Phillip Woods' age has been entered twice. You could add a new column that indicates the time of observation:

```{r}
people <- tribble(
  ~name,             ~key,    ~value, ~time,
  #-----------------|--------|------|-------
  "Phillip Woods",   "age",       45, 1,
  "Phillip Woods",   "height",   186, 1,
  "Phillip Woods",   "age",       50, 2,
  "Jessica Cordero", "age",       37, 1,
  "Jessica Cordero", "height",   156, 1
)
```

```{r}
people %>% spread(key, value)
```

4.  Tidy the simple tibble below. Do you need to spread or gather it?
    What are the variables?

    ```{r}
    preg <- tribble(
      ~pregnant, ~male, ~female,
      "yes",     NA,    10,
      "no",      20,    12
    )
    ```

It needs to be gathered. The variables are sex (male or female) and pregnant (yes or no).

```{r}
preg %>% gather(
  male, female,
  key = "sex",
  value = "count"
)
```

## 12.4.3 Exercises

1.  What do the `extra` and `fill` arguments do in `separate()`? 
    Experiment with the various options for the following two toy datasets.

    ```{r, eval = FALSE}
    tibble(x = c("a,b,c", "d,e,f,g", "h,i,j")) %>% 
      separate(x, c("one", "two", "three"))
    
    tibble(x = c("a,b,c", "d,e", "f,g,i")) %>% 
      separate(x, c("one", "two", "three"))
    ```

`extra` controls what happens when you use a character vector in `sep` and there are too many pieces.

```{r}
tibble(x = c("a,b,c", "d,e,f,g", "h,i,j")) %>% 
  separate(x, c("one", "two", "three"), extra = "merge")
```

In comparison, `fill` controls what happens when there are not enough pieces.

```{r}
tibble(x = c("a,b,c", "d,e", "f,g,i")) %>% 
  separate(x, c("one", "two", "three"), fill = "left")
```

2.  Both `unite()` and `separate()` have a `remove` argument. What does it
    do? Why would you set it to `FALSE`?

It removes the input column from the data frame. By setting this to `FALSE`, you can retain the original input column in the dataset:

```{r}
tibble(x = c("a,b,c", "d,e,f", "g,h,i")) %>% 
  separate(x, c("one", "two", "three"), remove = "FALSE")
```

3.  Compare and contrast `separate()` and `extract()`.  Why are there
    three variations of separation (by position, by separator, and with
    groups), but only one unite?

```{r}
tibble(x = c("a,b,c", "d,e,f", "g,h,i")) %>% 
  separate(x, c("one", "two", "three"))

tibble(x = c("a,b,c", "d,e,f", "g,h,i"))

tibble(x = c("a,b,c", "d,e,f", "g,h,i")) %>% 
  extract(x, "A")
```

`separate()` pulls apart one column and places the values into many columns. `extract()` is a similar function, but, as its name implies, only retains values you wish to extract from a data frame.

`unite()` only requires one variation of separation because it is combining multiple columns into one column. Therefore, there is only one way to do it (using `sep`). In comparison, there are multiple ways to split a character string for `separate()` (e.g., by position or by separator).

## 12.5.1 Exercises

1.  Compare and contrast the `fill` arguments to `spread()` and `complete()`.

For `spread()`, `fill` will replace missing values in a data set with this value. In comparison, the `fill` argument for `complete()` allows you to replace missing values by column name.

2.  What does the direction argument to `fill()` do?

It is the direction in which to fill missing values. For example:

```{r}
df <- data.frame(Month = 1:12, Year = c(NA, 2000, rep(NA, 10)))
df %>% fill(Year, .direction = c("up"))
```

## 12.6.1 Exercises

1.  In this case study I set `na.rm = TRUE` just to make it easier to
    check that we had the correct values. Is this reasonable? Think about
    how missing values are represented in this dataset. Are there implicit
    missing values? What's the difference between an `NA` and zero? 
    
I think this is reasonable. Missing values appear to be represented explicitely in this dataset:
    
```{r}
who %>%
  gather(new_sp_m014:newrel_f65, key = "key", value = "cases") %>% 
  count(is.na(cases))
```

There are over 76,000 `NA` values for `cases`.
    
It is difficult to determine whether there are implicit missing values in a dataset because, as stated earlier in the chapter, implicit missing values are an "absence of a presence".
    
`NA` indicates an explicit missing value. So, the number of actual cases for an `NA` observation is not necessarily zero. In comparison, a value of zero indicates there were no cases for a particular observation.
    
```{r}
who %>%
  gather(new_sp_m014:newrel_f65, key = "key", value = "cases") %>% 
  filter(cases == 0) %>% 
  nrow()
```
    
2.  What happens if you neglect the `mutate()` step?
    (`mutate(key = stringr::str_replace(key, "newrel", "new_rel"))`)
    
```{r}
who %>%
  gather(code, value, new_sp_m014:newrel_f65, na.rm = TRUE) %>% 
  separate(code, c("new", "var", "sexage")) %>% 
  select(-new, -iso2, -iso3) %>% 
  separate(sexage, c("sex", "age"), sep = 1)
```
    
It won't split the string `newrel` properly.
    
```{r}
split_problem <- who %>%
  gather(code, value, new_sp_m014:newrel_f65, na.rm = TRUE) %>% 
  separate(code, c("new", "var", "sexage")) %>% 
  select(-new, -iso2, -iso3) %>% 
  separate(sexage, c("sex", "age"), sep = 1)
```

```{r}
split_problem %>% slice(73467:73486)
```

For relapse cases, gender and age group have not separated (they appear under `var`), and `sex` and `age` variables are `NA`.

3.  I claimed that `iso2` and `iso3` were redundant with `country`. 
    Confirm this claim.

For each country, there is only one unique combination of `iso2` and `iso3`:

```{r}
who %>% 
  gather(new_sp_m014:newrel_f65, key = "key", value = "cases", na.rm = TRUE) %>% 
  select(country, iso2, iso3) %>% 
  distinct() %>% 
  group_by(country) %>% 
  filter(n() > 1)
```
    
4.  For each country, year, and sex compute the total number of cases of 
    TB. Make an informative visualisation of the data.
    
```{r}
who_cases <- who %>%
  gather(code, value, new_sp_m014:newrel_f65, na.rm = TRUE) %>% 
  mutate(code = stringr::str_replace(code, "newrel", "new_rel")) %>%
  separate(code, c("new", "var", "sexage")) %>% 
  select(-new, -iso2, -iso3) %>% 
  separate(sexage, c("sex", "age"), sep = 1) %>% 
  group_by(country, year, sex) %>% 
  summarise(cases = sum(value))

who_cases
```

```{r}
who_cases %>% 
  group_by(sex, year) %>%
  filter(year > 1995) %>% 
  summarise(cases_total = sum(cases)) %>%
  ggplot(aes(x = year, y = cases_total, group = sex, colour = sex)) +
    geom_line()
```

# 13. Relational data

```{r, include = FALSE}
library(maps)
```

## 13.2.1 Exercises

1.  Imagine you wanted to draw (approximately) the route each plane flies from
    its origin to its destination. What variables would you need? What tables
    would you need to combine?
    
From `flights`, you would need `origin` and `dest`. From `airports`, `lat`, `lon` (the locations of the airport), and maybe `alt` would be required. You would need to combine the `flights` and `airports` tables to get the origin airport and destination airport.

The `weather` table may also be helpful as wind speed, wind direction, and visibility may affect the route.

2.  I forgot to draw the relationship between `weather` and `airports`.
    What is the relationship and how should it appear in the diagram?
    
`weather` connects to `airports` via `origin` (the location). In the diagram, there should be an arrow pointing from `faa` in the `airports` table to `origin` in the `weather` table.

3.  `weather` only contains information for the origin (NYC) airports. If
    it contained weather records for all airports in the USA, what additional
    relation would it define with `flights`?
    
`year`, `month`, `day`, and `hour`.

4.  We know that some days of the year are "special", and fewer people than
    usual fly on them. How might you represent that data as a data frame?
    What would be the primary keys of that table? How would it connect to the
    existing tables?

You would have date and name variables. The primary key of this table would be date. It would match to `year`, `month`, and `day` in other tables such as `flights`.

## 13.3.1 Exercises

1.  Add a surrogate key to `flights`.

```{r}
flights %>% 
  arrange(year, month, day, sched_dep_time) %>% 
  mutate(flight_id = row_number())
```

2.  Identify the keys in the following datasets

    1.  `Lahman::Batting`,
    2.  `babynames::babynames`
    3.  `nasaweather::atmos`
    4.  `fueleconomy::vehicles`
    5.  `ggplot2::diamonds`
    
    (You might need to install some packages and read some documentation.)
    
`Lahman::Batting`:

The primary keys for `Lahman::Batting` are `playerID`, `yearID`, and `stint`.

```{r}
Lahman::Batting %>% 
  count(playerID, yearID, stint) %>% 
  filter(n > 1)
```

`playerID` is a foreign key (it is the primary key for `Lahman::Master`).

`babynames::babynames`:

The primary keys for `babynames::babynames` are `year`, `sex`, and `name`.

```{r}
babynames::babynames %>%
  group_by(year, sex, name) %>%
  filter(n() > 1)
```

`nasaweather::atmos`:

For `nasaweather:atmos`, the primary keys are `lat`, `long`, `year`, 
and `month`.

```{r}
nasaweather::atmos %>% 
  group_by(lat, long, year, month) %>% 
  filter(n() > 1)
```

`fueleconomy::vehicles`:

The primary key for `fueleconomy::vehicles` is `id`.

```{r}
fueleconomy::vehicles %>% 
  count(id) %>% 
  filter(n > 1)
```

`ggplot2::diamonds`:

`ggplot2::diamonds` does not appear to have a primary key.

```{r}
ggplot2::diamonds %>% 
  group_by(carat, cut, color, clarity, depth, table, price, x, y, z) %>% 
  filter(n() > 1) %>% 
  nrow()
```

3.  Draw a diagram illustrating the connections between the `Batting`,
    `Master`, and `Salaries` tables in the Lahman package. Draw another diagram
    that shows the relationship between `Master`, `Managers`, `AwardsManagers`.

    How would you characterise the relationship between the `Batting`,
    `Pitching`, and `Fielding` tables?
    
## 13.4.6 Exercises

1.  Compute the average delay by destination, then join on the `airports`
    data frame so you can show the spatial distribution of delays. Here's an
    easy way to draw a map of the United States:

    ```{r, eval = FALSE}
    airports %>%
      semi_join(flights, c("faa" = "dest")) %>%
      ggplot(aes(lon, lat)) +
        borders("state") +
        geom_point() +
        coord_quickmap()
    ```

    (Don't worry if you don't understand what `semi_join()` does --- you'll
    learn about it next.)

    You might want to use the `size` or `colour` of the points to display
    the average delay for each airport.
    
```{r}
dest_delay <- flights %>% 
  group_by(dest) %>% 
  summarise(avg_delay = mean(arr_delay, na.rm = TRUE))

dest_delay %>% 
  left_join(airports, c("dest" = "faa")) %>% 
  ggplot(aes(lon, lat)) +
    borders("state") +
    geom_point(aes(colour = avg_delay)) +
    coord_quickmap()
```

2.  Add the location of the origin _and_ destination (i.e. the `lat` and `lon`)
    to `flights`.
    
```{r}
flights %>% 
  left_join(airports, c("origin" = "faa")) %>% 
  left_join(airports, c("dest" = "faa")) %>% 
  rename(lat_origin = lat.x, lon_origin = lon.x) %>%
  rename(lat_dest = lat.y, lon_dest = lon.y) %>% 
  select(-contains("."))
```

3.  Is there a relationship between the age of a plane and its delays?

```{r}
planes_avg_delay <- planes %>% 
  select(tailnum, year) %>% 
  left_join(flights, "tailnum") %>% 
  group_by(tailnum, year.x) %>%
  rename(year = year.x) %>% 
  summarise(avg_delay = mean(arr_delay, na.rm = TRUE))

ggplot(planes_avg_delay, aes(year, avg_delay)) +
  geom_point() +
  geom_smooth()
```

It appears not.

4.  What weather conditions make it more likely to see a delay?

I think rain and low visibility would increase the likelihood of delay.

```{r}
flights_weather <- flights %>% 
  inner_join(weather, c("origin", "year", "month", "day", "hour"))

flights_weather %>% 
  group_by(visib) %>% 
  summarise(avg_delay = mean(dep_delay, na.rm = TRUE)) %>% 
  ggplot(aes(visib, avg_delay)) + 
    geom_point() +
    geom_line() +
    geom_smooth(method = lm, se = FALSE)
```

As visibility increases, the average delay decreases.

```{r}
flights_weather %>% 
  group_by(precip) %>% 
  summarise(avg_delay = mean(dep_delay, na.rm = TRUE)) %>% 
  ggplot(aes(precip, avg_delay)) + 
    geom_point() +
    geom_line() +
    geom_smooth(method = lm, se = FALSE)
```

The relationship between rainfall and average delay is less clear.

5.  What happened on June 13 2013? Display the spatial pattern of delays,
    and then use Google to cross-reference with the weather.
    
There were a large series of storms in the south-east.

    ```{r, eval = FALSE, include = FALSE}
    worst <- filter(flights, !is.na(dep_time), month == 6, day == 13)
    worst %>%
      group_by(dest) %>%
      summarise(delay = mean(arr_delay), n = n()) %>%
      filter(n > 5) %>%
      inner_join(airports, by = c("dest" = "faa")) %>%
      ggplot(aes(lon, lat)) +
        borders("state") +
        geom_point(aes(size = n, colour = delay)) +
        coord_quickmap()
    ```
    
```{r}
flights %>% 
  filter(month == 6, day == 13) %>% 
  group_by(hour) %>% 
  summarise(avg_delay = mean(dep_delay, na.rm = TRUE))
```

By mid-morning there were significant delays. This continued into the evening.

```{r}
flights %>% 
  filter(month == 6, day == 13) %>% 
  group_by(dest) %>% 
  summarise(avg_delay = mean(dep_delay, na.rm = TRUE)) %>% 
  inner_join(airports, by = c("dest" = "faa")) %>%
    ggplot(aes(lon, lat)) +
      borders("state") +
      geom_point(aes(colour = avg_delay, size = avg_delay)) +
      coord_quickmap()
```

## 13.5.1 Exercises

1.  What does it mean for a flight to have a missing `tailnum`? What do the
    tail numbers that don't have a matching record in `planes` have in common?
    (Hint: one variable explains ~90% of the problems.)
    
```{r}
flights %>% 
  anti_join(planes, by = "tailnum")
```

Many of the flights without a tail number in `planes` appear to be American Airlines (`AA`) or Envoy Air (`MQ`).
    
```{r}
flights %>% 
  anti_join(planes, by = "tailnum") %>% 
  group_by(carrier) %>% 
  summarise(count = n())
```

```{r}
flights %>% 
  anti_join(planes, by = "tailnum") %>% 
  nrow()
```

Specifically, about 90% of flights.

2.  Filter flights to only show flights with planes that have flown at least 100
    flights.
    
```{r}
flights_100 <- flights %>% 
  group_by(tailnum) %>% 
  count() %>% 
  filter(n >= 100 & tailnum != "NA") %>% 
  arrange(desc(n))

flights %>% 
  semi_join(flights_100, by = "tailnum")
```

3.  Combine `fueleconomy::vehicles` and `fueleconomy::common` to find only the
    records for the most common models.
    
```{r}
fueleconomy::vehicles %>% 
  semi_join(fueleconomy::common, by = c("make", "model"))

```

4.  Find the 48 hours (over the course of the whole year) that have the worst
    delays. Cross-reference it with the `weather` data. Can you see any
    patterns?
    
5.  What does `anti_join(flights, airports, by = c("dest" = "faa"))` tell you?
    What does `anti_join(airports, flights, by = c("faa" = "dest"))` tell you?
    
```{r}
anti_join(flights, airports, by = c("dest" = "faa"))
```

The destination of these flights is unknown (i.e., the airport does not appear in `airports`). They could be international flights.

```{r}
anti_join(airports, flights, by = c("faa" = "dest"))
```

These are the destination airports for all flights that departed NYC in 2013.

6.  You might expect that there's an implicit relationship between plane
    and airline, because each plane is flown by a single airline. Confirm
    or reject this hypothesis using the tools you've learned above.
    
```{r}
flights_carrier <- flights %>% 
  group_by(tailnum, carrier) %>% 
  count() %>% 
  arrange(tailnum)
flights_carrier

flights_carrier %>% 
  group_by(tailnum) %>% 
  count() %>% 
  filter(nn > 1)
```

There are a number of planes that have been flown by multiple airlines. However, the majority of planes have had a single airline.

# 14. Strings

To come...

# 15. Factors

```{r, include = FALSE}
library(forcats)
```

## 15.3.1 Exercises

1.  Explore the distribution of `rincome` (reported income). What makes the
    default bar chart hard to understand? How could you improve the plot?
    
```{r}
summary(gss_cat$rincome)
```

```{r}
ggplot(gss_cat, aes(rincome)) +
  geom_bar()
```

This plot can be improved by rotating the labels on the x-axis:

```{r}
ggplot(gss_cat, aes(rincome)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 35, hjust = 1))
```

It would also be better if the categories on the x-axis were ordered from smallest to largest. To do this, you would have to modify the order of the factor levels. The following categories can also be combined: "No answer", "Don't know", "Refused", and "Not applicable".

2.  What is the most common `relig` in this survey? What's the most
    common `partyid`?

```{r}
gss_cat %>% 
  count(relig) %>% 
  arrange(desc(n)) %>% 
  head(1)

gss_cat %>% 
  count(partyid) %>% 
  arrange(desc(n)) %>% 
  head(1)
```

"Protestant"" is the most common religion.

The most common party affiliation is "Independent".

3.  Which `relig` does `denom` (denomination) apply to? How can you find
    out with a table? How can you find out with a visualisation?  
    
```{r}
levels(gss_cat$denom)

no_denom <- gss_cat %>%
  filter(!denom %in% c("No answer", "Don't know", "No denomination", "Not applicable", "Refused"))
```

```{r}
no_denom %>% 
  count(relig)
```

```{r}
ggplot(no_denom, aes(relig, denom)) +
  geom_count(aes(colour = ..n.., size = ..n..)) +
  guides(colour = 'legend')
```

Denomination applies to "Protestant".

## 15.4.1 Exercises

1.  There are some suspiciously high numbers in `tvhours`. Is the mean a good
    summary?
    
```{r}
summary(gss_cat$tvhours)

ggplot(gss_cat, aes(tvhours)) +
  geom_bar()

gss_cat %>% 
  count(tvhours) %>% 
  arrange(desc(tvhours))
```

It is unlikely that people are watching 24 hours of tv per day. As seen in the bar plot, `tvhours` is skewed to the right due to the presence of these extreme values. The median would be a better summary of the data, because, unlike the mean, it is largely unaffected by outliers.

2.  For each factor in `gss_cat` identify whether the order of the levels is
    arbitrary or principled.

```{r}
glimpse(gss_cat)

levels(gss_cat$marital)
levels(gss_cat$rincome)
```

`marital`, `race` `partyid`, `relig`, and `denom` are arbitrary (i.e., nominal). `rincome` is principled (i.e., ordinal).

3.  Why did moving "Not applicable" to the front of the levels move it to the
    bottom of the plot?
    
```{r}
rincome_summary <- gss_cat %>%
  group_by(rincome) %>%
  summarise(
    age = mean(age, na.rm = TRUE),
    tvhours = mean(tvhours, na.rm = TRUE),
    n = n()
  )

ggplot(rincome_summary, aes(age, fct_relevel(rincome, "Not applicable"))) +
  geom_point()
```

Because `fct_relevel` gives "Not applicable" an integer value of 1:

```{r}
rincome_levels <- fct_relevel(rincome_summary$rincome, "Not applicable")
levels(rincome_levels)
```

## 15.5.1 Exercises

1.  How have the proportions of people identifying as Democrat, Republican, and
    Independent changed over time?
    
```{r}
partyid_clean <- gss_cat %>%
  mutate(partyid = fct_collapse(
    partyid,
    other = c("No answer", "Don't know", "Other party"),
    rep = c("Strong republican", "Not str republican"),
    ind = c("Ind,near rep", "Independent", "Ind,near dem"),
    dem = c("Not str democrat", "Strong democrat")
  ))

partyid_counts <- partyid_clean %>% 
  count(year, partyid) %>% 
  group_by(year) %>% 
  mutate(prop = n / sum(n))

ggplot(partyid_counts, aes(x = year, y = prop, colour = fct_reorder2(
  partyid, year, prop
))) +
  geom_point() +
  geom_line() +
  labs(colour = "partyid")
```
    
There appears to be less people identifying as Republican over time. In comparison, the proportion of people identifying as Independent appears to be increasing, while the number of Democrats appear about the same.

There is also a slight increase in the number of people identifying as Other since around 2005.

2.  How could you collapse `rincome` into a small set of categories?

```{r}
gss_cat %>% 
  count(rincome)
```

```{r}
gss_cat %>%
  mutate(rincome = fct_lump(rincome, n = 5)) %>%
  count(rincome, sort = TRUE) %>%
  mutate(rincome = fct_recode(
    rincome,
    "Less than $10000" = "Other"
  ))
```

# 16. Dates and times

```{r, include = FALSE}
library(lubridate)
library(nycflights13)
library(forcats)
```

## 16.2.4 Exercises

1.  What happens if you parse a string that contains invalid dates?

    ```{r, eval = FALSE}
    ymd(c("2010-10-10", "bananas"))
    ```
    
The invalid dates will be `NA`.

2.  What does the `tzone` argument to `today()` do? Why is it important?

`tzone` specifies which time zone you would like to find the current date of.

3.  Use the appropriate lubridate function to parse each of the following dates:

    ```{r}
    d1 <- "January 1, 2010"
    d2 <- "2015-Mar-07"
    d3 <- "06-Jun-2017"
    d4 <- c("August 19 (2015)", "July 1 (2015)")
    d5 <- "12/30/14" # Dec 30, 2014
    ```

```{r}
mdy(d1)
ymd(d2)
dmy(d3)
mdy(d4)
mdy(d5)
```

## 16.3.4 Exercises

1.  How does the distribution of flight times within a day change over the 
    course of the year?
    
```{r, include = FALSE}
make_datetime_100 <- function(year, month, day, time) {
  make_datetime(year, month, day, time %/% 100, time %% 100)
}

flights_dt <- flights %>% 
  filter(!is.na(dep_time), !is.na(arr_time)) %>% 
  mutate(
    dep_time = make_datetime_100(year, month, day, dep_time),
    arr_time = make_datetime_100(year, month, day, arr_time),
    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)
  ) %>% 
  select(origin, dest, ends_with("delay"), ends_with("time"))
```


```{r}
flights_dt %>% 
  mutate(hour = hour(dep_time), month = factor(month(dep_time))) %>% 
  group_by(hour, month) %>% 
  summarise(
    n = n()
  ) %>% 
  ggplot(aes(x = hour, y = n, colour = month)) +
    geom_line()
```

For the first day of each month it looks about the same.
    
2.  Compare `dep_time`, `sched_dep_time` and `dep_delay`. Are they consistent?
    Explain your findings.

3.  Compare `air_time` with the duration between the departure and arrival.
    Explain your findings. (Hint: consider the location of the airport.)
    
4.  How does the average delay time change over the course of a day?
    Should you use `dep_time` or `sched_dep_time`? Why?

5.  On what day of the week should you leave if you want to minimise the
    chance of a delay?
    
```{r}
flights_dt %>% 
  mutate(day = wday(sched_dep_time, label = TRUE)) %>% 
  group_by(day) %>% 
  summarise(dep_delay = mean(dep_delay)) %>%
  ggplot(aes(day, dep_delay)) +
    geom_col()
```
    
Saturday.

6.  What makes the distribution of `diamonds$carat` and 
    `flights$sched_dep_time` similar?

7.  Confirm my hypothesis that the early departures of flights in minutes
    20-30 and 50-60 are caused by scheduled flights that leave early. 
    Hint: create a binary variable that tells you whether or not a flight 
    was delayed.
    
# 19. Functions

## 19.2.1 Practice

1.  Why is `TRUE` not a parameter to `rescale01()`? What would happen if
    `x` contained a single missing value, and `na.rm` was `FALSE`?
    
Because it is not an input. The function takes only one input (`x`) for computing. `na.rm` specifies whether `NA` values should be removed or not prior to the calculation.

```{r}
rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}

rescale01(c(0, 5))
```

So far so good.

```{r}
rescale01 <- function(x) {
  rng <- range(x, na.rm = FALSE)
  (x - rng[1]) / (rng[2] - rng[1])
}

rescale01(c(0, 5, NA))
```

When `na.rm` is `FALSE`, `NA` values are contagious.

2.  In the second variant of `rescale01()`, infinite values are left
    unchanged. Rewrite `rescale01()` so that `-Inf` is mapped to 0, and 
    `Inf` is mapped to 1.
    
```{r}
x <- c(1:10, Inf)

rescale01_remapped <- function(x) {
  rng <- range(x, na.rm = TRUE, finite = TRUE)
  y <- (x - rng[1]) / (rng[2] - rng[1])
  y[y == -Inf] <- 0
  y[y == Inf] <- 1
  y
}

rescale01_remapped(x)
```

3.  Practice turning the following code snippets into functions. Think about 
    what each function does. What would you call it? How many arguments does it
    need? Can you rewrite it to be more expressive or less duplicative?

    ```{r, eval = FALSE}
    mean(is.na(x))
    
    x / sum(x, na.rm = TRUE)
    
    sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)
    ```
    
`mean()` calculates the mean of an object. `is.na()` checks whether there are any missing values in an object and returns a logical vector.

```{r}
x <- c(1, 5, 10, 25, 25)
mean(x)

y <- c(2, 5, NA, NA)
```

Therefore, `mean(is.na(x))` calculates the mean of a logical vector, where `TRUE` is equal to 1, and `FALSE` is equal to 0.

So, this function calculates the proportion of missing values in an object. It can be turned into a function like this:

```{r}
prop_NA <- function(x){
  mean(is.na(x))
}

x <- c(1, 10, NA, NA)

prop_NA(x)
```

```{r}
x <- c(1, 5, 10, 25, 25)
x / sum(x, na.rm = TRUE)
```

The code above calculates the proportion of each value in a numeric vector by dividing each value by the total sum of the vector. It also removes any missing values prior to calculation.

```{r}
prop <- function(x){
  x / sum(x, na.rm = TRUE)
}

prop(x)
```

```{r}
sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)
```

Finally, the last snippet of code divides the standard deviation of a numeric vector by the mean. According to Wikipedia, this is known as the coefficient of variation:

> ... the coefficient of variation, also known as relative standard deviation, is a standardized measure of dispersion of a probability distribution or frequency distribution.

It can be turned into a function like this:

```{r}
coeff_var <- function(x){
  sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)
}

coeff_var(x)
```

4.  Follow <http://nicercode.github.io/intro/writing-functions.html> to 
    write your own functions to compute the variance and skew of a numeric 
    vector.

```{r}
# Variance
x <- sample(1:100, 100, replace = TRUE)

sample_var <- function(x){
n <- length(x)
m <- mean(x)
(1 / (n - 1)) * sum((x - m) ^ 2)
}
sample_var(x)
# Computes the same result as the built-in function:
var(x)
```

```{r}
# Skew
x <- sample(1:100, 100, replace = TRUE)

skew <- function(x){
  n <- length(x)
  m <- mean(x)
  sd <- sd(x)
(1 / n) * sum(x - m) ^ 3 / sd ^ 3
}
skew(x)
```

5.  Write `both_na()`, a function that takes two vectors of the same length 
    and returns the number of positions that have an `NA` in both vectors.
    
```{r}
x <- c(1, 4, 2, NA, NA)
y <- c(3, 5, NA, 1, NA)

both_na <- function(x, y){
  sum(is.na(x) & is.na(y))
}
```

6.  What do the following functions do? Why are they useful even though they
    are so short?
    
    ```{r}
    is_directory <- function(x) file.info(x)$isdir
    is_readable <- function(x) file.access(x, 4) == 0
    ```


```{r}
is_directory("~/Google Drive/R/r4ds-solutions/data")
is_readable("index.html")
```

`is_directory()` can determine whether a file path is a directory. `is_readable()` tests whether a particular file is readable or not. These functions are helpful because the function names make it more readily apparent what each function does without the user having to look through the code.

7.  Read the [complete lyrics](https://en.wikipedia.org/wiki/Little_Bunny_Foo_Foo) 
    to "Little Bunny Foo Foo". There's a lot of duplication in this song. 
    Extend the initial piping example to recreate the complete song, and use 
    functions to reduce the duplication.
    
```{r, eval=FALSE}
foo_foo_mischief <- function(x){
foo_foo %>%
  hop(through = forest) %>%
  scoop(up = field_mouse) %>%
  bop(on = head)
}

foo_foo_mischief()
good_fairy %>% 
  says("Little bunny Foo Foo
        I don't want to see you
        Scooping up the field mice
        And bopping them on the head.
        I'll give you three chances,
        And if you don't stop, I'll turn you into a GOON!")
```

## 19.3.1 Exercises

1.  Read the source code for each of the following three functions, puzzle out
    what they do, and then brainstorm better names.
    
    ```{r}
    f1 <- function(string, prefix) {
      substr(string, 1, nchar(prefix)) == prefix
    }
    f2 <- function(x) {
      if (length(x) <= 1) return(NULL)
      x[-length(x)]
    }
    f3 <- function(x, y) {
      rep(y, length.out = length(x))
    }
    ```

The first function checks whether a string has a given prefix.

```{r}
f1("hello world", "hello")
```

`substr()` extracts substrings from a character vector by starting from the first element (`1`), and by using the number of characters in the prefix as the last/stop value. Then, it evaluates whether the substring is equal to that of the prefix.

```{r}
substr("hello world", 1, nchar("hello"))
"hello" == "hello"
```

A better name for this function might be: 

```{r}
check_prefix <- function(string, prefix) {
  substr(string, 1, nchar(prefix)) == prefix
}
```

The second function removes the last observation in a vector (this doesn't necessarily have to be an integer). For example:

```{r}
remove_last <- function(x) {
  if (length(x) <= 1) return(NULL)
  x[-length(x)]
}

x <- sample(1:10, 10, replace = TRUE)
y <- c("hello", "world")

remove_last(x)
remove_last(y)
```

The last function replicates the values in y based on the length of x.

```{r}
repeat_values <- function(x, y) {
  rep(y, length.out = length(x))
}

x <- c(1, 3, 5, 3)
y <- c("hello", "world")

repeat_values(x, y)
```

2.  Take a function that you've written recently and spend 5 minutes 
    brainstorming a better name for it and its arguments.
    
```{r}
df <- tibble(
  a = c("47%", "23%", "97%", "88%", "12%")
)

remove_percentage <- function(column) {
  parse_double(gsub("[%]", "", column))
}

df$a <- remove_percentage(df$a)
df
```

3.  Compare and contrast `rnorm()` and `MASS::mvrnorm()`. How could you make
    them more consistent?

`rnorm()` produces random numbers that are normally distributed with a mean equal to 0 and a standard deviation equal to 1. In comparison, `MASS::mvrnorm()` produces samples from a multivariate normal distribution. To make them more consistent, they should use the same argument names. For example, `MASS::mvrnorm()` uses `mu`, where as `rnorm()` uses `mean`. Using consistent names between the functions would make them easier to use.
    
4.  Make a case for why `norm_r()`, `norm_d()` etc would be better than
    `rnorm()`, `dnorm()`. Make a case for the opposite.
    
Using `norm_r()`, `norm_d()` instead of  `rnorm()`, `dnorm()` would be better because it would group the functions related to the normal distribution. In comparison, using `rnorm()`, `dnorm()` could be better because it groups them by actions. For example, `r*` functions involve sampling from distributions.
    
## 19.4.4 Exercises

1.  What's the difference between `if` and `ifelse()`? Carefully read the help
    and construct three examples that illustrate the key differences.

2.  Write a greeting function that says "good morning", "good afternoon",
    or "good evening", depending on the time of day. (Hint: use a time
    argument that defaults to `lubridate::now()`. That will make it 
    easier to test your function.)

3.  Implement a `fizzbuzz` function. It takes a single number as input. If
    the number is divisible by three, it returns "fizz". If it's divisible by
    five it returns "buzz". If it's divisible by three and five, it returns
    "fizzbuzz". Otherwise, it returns the number. Make sure you first write 
    working code before you create the function.
    
4.  How could you use `cut()` to simplify this set of nested if-else statements?

    ```{r, eval = FALSE}
    if (temp <= 0) {
      "freezing"
    } else if (temp <= 10) {
      "cold"
    } else if (temp <= 20) {
      "cool"
    } else if (temp <= 30) {
      "warm"
    } else {
      "hot"
    }
    ```
    
    How would you change the call to `cut()` if I'd used `<` instead of `<=`?
    What is the other chief advantage of `cut()` for this problem? (Hint:
    what happens if you have many values in `temp`?)

5.  What happens if you use `switch()` with numeric values?

6.  What does this `switch()` call do? What happens if `x` is "e"?

    ```{r, eval = FALSE}
    switch(x, 
      a = ,
      b = "ab",
      c = ,
      d = "cd"
    )
    ```
    
    Experiment, then carefully read the documentation.
    
# 21. Iteration

## 21.2.1 Exercises

1.  Write for loops to:

    1. Compute the mean of every column in `mtcars`.
    2. Determine the type of each column in `nycflights13::flights`.
    3. Compute the number of unique values in each column of `iris`.
    4. Generate 10 random normals for each of $\mu = -10$, $0$, $10$, and $100$.
    
    Think about the output, sequence, and body __before__ you start writing
    the loop.
    
Compute the mean of every column in `mtcars`:

```{r}
mean <- vector("double", ncol(mtcars))
names(mean) <- names(mtcars)
for (i in seq_along(mtcars)) {
  mean[[i]] <- mean(mtcars[[i]])
}
mean
```

Determine the type of each column in `nycflights13::flights`:

```{r}
col_type <- vector("list", ncol(flights))
names(col_type) <- names(flights)
for (i in seq_along(flights)) {
  col_type[[i]] <- class(flights[[i]])
}
col_type
```

Compute the number of unique values in each column of `iris`:

```{r}
unique_values <- vector("integer", ncol(iris))
names(unique_values) <- names(iris)
for (i in seq_along(iris)) {
  unique_values[[i]] <- n_distinct(iris[[i]])
}
unique_values

# Check the above results
fct_unique(iris$Species)
```

Generate 10 random normals for each of $\mu = -10$, $0$, $10$, and $100$:

```{r}
mu <- c(-10, 0, 10, 100)

random_normals <- vector("list", length(mu))
for (i in seq_along(random_normals)) {
  random_normals[[i]] <- rnorm(10, mean = mu[[i]])
}
random_normals
```

2.  Eliminate the for loop in each of the following examples by taking 
    advantage of an existing function that works with vectors:
    
    ```{r, eval = FALSE}
    out <- ""
    for (x in letters) {
      out <- stringr::str_c(out, x)
    }
    
    x <- sample(100)
    sd <- 0
    for (i in seq_along(x)) {
      sd <- sd + (x[i] - mean(x)) ^ 2
    }
    sd <- sqrt(sd / (length(x) - 1))
    
    x <- runif(100)
    out <- vector("numeric", length(x))
    out[1] <- x[1]
    for (i in 2:length(x)) {
      out[i] <- out[i - 1] + x[i]
    }
    ```

3.  Combine your function writing and for loop skills:

    1. Write a for loop that `prints()` the lyrics to the children's song 
       "Alice the camel".

    2. Convert the nursery rhyme "ten in the bed" to a function. Generalise 
       it to any number of people in any sleeping structure.

    3. Convert the song "99 bottles of beer on the wall" to a function.
       Generalise to any number of any vessel containing any liquid on 
       any surface.
       
4.  It's common to see for loops that don't preallocate the output and instead
    increase the length of a vector at each step:
    
    ```{r, eval = FALSE}
    output <- vector("integer", 0)
    for (i in seq_along(x)) {
      output <- c(output, lengths(x[[i]]))
    }
    output
    ```
    
    How does this affect performance? Design and execute an experiment.
    
Preallocated loops perform increasingly better as inputs increase in size.
    
# 27. R Markdown

## 27.2.1 Exercises

1.  Create a new notebook using _File > New File > R Notebook_. Read the 
    instructions. Practice running the chunks. Verify that you can modify
    the code, re-run it, and see modified output.
    
2.  Create a new R Markdown document with _File > New File > R Markdown..._
    Knit it by clicking the appropriate button. Knit it by using the 
    appropriate keyboard short cut. Verify that you can modify the
    input and see the output update.
    
3.  Compare and contrast the R notebook and R markdown files you created
    above. How are the outputs similar? How are they different? How are
    the inputs similar? How are they different? What happens if you
    copy the YAML header from one to the other?
    
* R notebook files are suitable for capturing what you did, what you were
thinking, and communicating to other data scientists
* R markdown files are suitable for communicating to decision-makers who
don't care about the code
* An R notebook file allows the user to show/hide code and download the file;
an R markdown file does not
* The R markdown file contains additional information in the YAML header
(author and date) by default
* When you copy the YAML header from one to the other it changes the type of
file
    + `output:` determines the type of file that will be created

4.  Create one new R Markdown document for each of the three built-in
    formats: HTML, PDF and Word. Knit each of the three documents.
    How does the output differ? How does the input differ? (You may need
    to install LaTeX in order to build the PDF output --- RStudio will
    prompt you if this is necessary.)
    
Generally, each output will differ visually. For example, the html output uses a sans-serif font while the pdf output uses a serif font.

## 27.3.1 Exercises

1.  Practice what you've learned by creating a brief CV. The title should be
    your name, and you should include headings for (at least) education or
    employment. Each of the sections should include a bulleted list of
    jobs/degrees. Highlight the year in bold.
    
See [this repo](https://github.com/benherbertson/cv).
    
2.  Using the R Markdown quick reference, figure out how to:

    1.  Add a footnote.
    1.  Add a horizontal rule.
    1.  Add a block quote.
    
Add a footnote:

```{r, eval=FALSE}
A footnote[^1]

[^1]: Here is the footnote
```

A footnote[^1]

(This footnote will be displayed at the bottom of this document.)

[^1]: Here is the footnote  

Add a horizontal rule:

```{r, eval=FALSE}
***
```

***

Add a block quote:
```{r, eval=FALSE}
> block quote
```

> block quote

3.  Copy and paste the contents of `diamond-sizes.Rmd` from
    <https://github.com/hadley/r4ds/tree/master/rmarkdown> in to a local
    R markdown document. Check that you can run it, then add text after the 
    frequency polygon that describes its most striking features.

# 28. Graphics for communication

## 28.2.1 Exercises

1.  Create one plot on the fuel economy data with customised `title`,
    `subtitle`, `caption`, `x`, `y`, and `colour` labels.
    
```{r}
ggplot(mpg, aes(x = hwy, y = cty)) +
  geom_point(aes(colour = class)) +
  labs(
    title = "Small cars are more fuel efficient",
    subtitle = "SUVs have poor fuel economy",
    caption = "Data from fueleconomy.gov",
    x = "Highway miles per gallon",
    y = "City miles per gallon",
    colour = "Class"
  )
```

2.  The `geom_smooth()` is somewhat misleading because the `hwy` for
    large engines is skewed upwards due to the inclusion of lightweight
    sports cars with big engines. Use your modelling tools to fit and display
    a better model.

3.  Take an exploratory graphic that you've created in the last month, and add
    informative titles to make it easier for others to understand.
    
```{r}  
ggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +
  geom_point(aes(colour = Species)) +
  labs(
    title = "Iris virginica and versicolor have large petals",
    subtitle = "Iris setosa have substantially smaller petals",
    caption = "Source: Edgar Anderson's Iris Data",
    x = "Petal length (cm)",
    y = "Petal width (cm)",
    colour = "Species of iris"
  )
```

## 28.3.1 Exercises

1.  Use `geom_text()` with infinite positions to place text at the
    four corners of the plot.
    
```{r}
labels <- tibble(
  cty = c(Inf, Inf, -Inf, -Inf),
  displ = c(Inf, -Inf, Inf, -Inf),
  labels = c("Text", "tExt", "teXt", "texT"),
  vjust_var = c("top", "top", "bottom", "bottom"),
  hjust_var = c("right", "left", "right", "left")
)

ggplot(mpg, aes(x = displ, y = cty)) +
  geom_point(aes(colour = class)) +
  geom_text(data = labels, aes(label = labels, vjust = vjust_var, hjust = hjust_var))
```

2.  Read the documentation for `annotate()`. How can you use it to add a text
    label to a plot without having to create a tibble?
    
```{r}
ggplot(mpg, aes(x = displ, y = cty)) +
  geom_point(aes(colour = class)) +
  annotate("text", x = 7, y = 35, label = "Text")
```

You can also easily add multiple labels:

```{r}
ggplot(mpg, aes(x = displ, y = cty)) +
  geom_point(aes(colour = class)) +
  annotate("text", x = c(7, 1), y = c(35, 35), label = c("Text", "tExt"))
```

3.  How do labels with `geom_text()` interact with faceting? How can you
    add a label to a single facet? How can you put a different label in
    each facet? (Hint: think about the underlying data.)
    
4.  What arguments to `geom_label()` control the appearance of the background
    box?
    
The `fill` aesthetic. For example:

```{r}
best_in_class <- mpg %>%
  group_by(class) %>%
  filter(row_number(desc(hwy)) == 1)

ggplot(mpg, aes(x = displ, y = cty)) +
  geom_point(aes(colour = class)) +
  geom_label(aes(label = model, fill = class), data = best_in_class)
```

5.  What are the four arguments to `arrow()`? How do they work? Create a series
    of plots that demonstrate the most important options.
    
The four arguments to `arrow()` are angle, length, ends, and type. `geom_segment` has an arrow argument.